{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier: diabetes prediction\n",
    "\n",
    "Absolutely minimal MVP (minimum viable product) solution.\n",
    "\n",
    "## 1. Data acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle imports up-front\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import uniform, norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the URL\n",
    "data_df=pd.read_csv('https://raw.githubusercontent.com/4GeeksAcademy/decision-tree-project-tutorial/main/diabetes.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from labels\n",
    "labels=data_df['Outcome']\n",
    "features=data_df.drop('Outcome', axis=1)\n",
    "\n",
    "# Split the data into training and testing features and labels\n",
    "training_features, testing_features, training_labels, testing_labels=train_test_split(\n",
    "    features,\n",
    "    labels,\n",
    "    test_size=0.25,\n",
    "    random_state=315\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "\n",
    "### 2.1. Baseline model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a reusable helper function for cross-validation here. We are going to\n",
    "# be doing a lot of cross-validation, this allows us to reuse this code\n",
    "# without having to copy-paste it over and over.\n",
    "\n",
    "def cross_val(model, features: pd.DataFrame, labels: pd.Series) -> list[float]:\n",
    "    '''Reusable helper function to run cross-validation on a model. Takes model,\n",
    "    Pandas data frame of features and Pandas data series of labels. Returns \n",
    "    list of cross-validation fold accuracy scores as percents.'''\n",
    "\n",
    "    # Define the cross-validation strategy\n",
    "    cross_validation=StratifiedKFold(n_splits=7, shuffle=True, random_state=315)\n",
    "\n",
    "    # Run the cross-validation, collecting the scores\n",
    "    scores=cross_val_score(\n",
    "        model,\n",
    "        features,\n",
    "        labels,\n",
    "        cv=cross_validation,\n",
    "        n_jobs=-1,\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "\n",
    "    # Print mean and standard deviation of the scores\n",
    "    print(f'Cross-validation accuracy: {(scores.mean() * 100):.2f} +/- {(scores.std() * 100):.2f}%')\n",
    "\n",
    "    # Return the scores\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a random forest classifier model\n",
    "model=RandomForestClassifier(random_state=315)\n",
    "\n",
    "# Run the cross-validation\n",
    "scores=cross_val(model, training_features, training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Missing, and/or extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model optimization\n",
    "### 4.1. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define the hyperparameter distributions to sample from\n",
    "distributions={\n",
    "    'n_estimators': list(range(2, 100))\n",
    "}\n",
    "\n",
    "# Instantiate a random forest classifier model\n",
    "model=RandomForestClassifier(random_state=315)\n",
    "\n",
    "# Define the cross-validation strategy\n",
    "cross_validation=StratifiedKFold(n_splits=7, shuffle=True, random_state=315)\n",
    "\n",
    "# Set-up the search\n",
    "search=RandomizedSearchCV(\n",
    "    model,\n",
    "    distributions,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=cross_validation,\n",
    "    n_iter=100,\n",
    "    random_state=315,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "# Run the grid search\n",
    "results=search.fit(training_features, training_labels)\n",
    "\n",
    "# Print the best parameter settings found at the end\n",
    "print(f'Best hyperparameters: {results.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Cross-validation of optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Final model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. A note on interpretation & some statistics\n",
    "\n",
    "As a professional data scientist, in addition to making the model as accurate as possible, we want to have a good estimate of how accurate it will be on un-seen test data. With our cross-validation results, we can take advantage of some simple statistics to talk about probabilities and confidence intervals. \n",
    "\n",
    "### 5.1. Confidence intervals on model performace\n",
    "\n",
    "The first useful thing to look at is a confidence interval around the cross-validation performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound, upper_bound=norm.interval(0.95, loc=scores.mean(), scale=scores.std())\n",
    "print(f'95% CI = {lower_bound*100:.1f}% - {upper_bound*100:.1f}% accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Likelihood of test results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use SciPy's stats module to calculate the probability of the test set result we observed, given our cross-validation results. Doing so will give us a nice way to quantify how well we are estimating true out-of-sample performance. If our test score is likely, we are in good shape. If the test result is very unlikely then something is probably wrong.\n",
    "\n",
    "For example, if our test result is 76.0% accuracy, did we do a good job estimating out-of-sample performance with cross-validation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your test set accuracy here\n",
    "testing_percent_accuracy=76.0\n",
    "\n",
    "# Convert the test accuracy to a z-score using the mean and standard \n",
    "# deviation from the cross-validation\n",
    "z_score=((testing_percent_accuracy/100) - scores.mean()) / scores.std()\n",
    "\n",
    "# Use the standard normal distribution's probability density function to\n",
    "# get the probability of observing our testing accuracy score\n",
    "probability=norm.pdf(z_score)\n",
    "\n",
    "print(f'Probability: {probability*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example - what would happen if our test result was only 68.0% accurate - just outside of our 95% confidence interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_percent_accuracy=68.0\n",
    "\n",
    "z_score=((testing_percent_accuracy/100) - scores.mean()) / scores.std()\n",
    "probability=norm.pdf(z_score)\n",
    "\n",
    "print(f'Probability: {probability*100:.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
